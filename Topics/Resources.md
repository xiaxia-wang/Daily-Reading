





*2025-05-13*

#### [GLoRa: A Benchmark to Evaluate the Ability to Learn Long-Range Dependencies in Graphs](https://openreview.net/forum?id=2jf5x5XoYk)

*Dongzhuoran Zhou, Evgeny Kharlamov, Egor V. Kostylev*

*ICLR 2025*

The paper presents an algorithm for generating a synthetic dataset for varied dependency lengths and demonstrates how to use this benchmark to identify, with certain guarantees, the maximum dependency length that a graph learning system can learn.


*2025-05-12*

#### [Reliable and diverse evaluation of LLM medical knowledge mastery](https://openreview.net/forum?id=TXfzH933qV)

*Yuxuan Zhou, Xien Liu, Chen Ning, Xiao Zhang, Ji Wu*

*ICLR 2025*

This paper proposes a benchmark for evaluating LLMs' medical knowledge proficiency, which generates reliable and diverse test samples by predicate equivalence transformations.


*2025-05-09*

#### [MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation](https://openreview.net/forum?id=br4H61LOoI)

*Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia*

*ICLR 2025*

(Copied from metareview) The paper is interested in shifting the task of an LLM, from producing a solution to a query, to evaluating a pair (query, answer): scoring its correctness and possibly identifying where there is a failure and why. A most interesting part of the discussion is whether evaluating solutions and producing them are mostly equivalent skills.


*2025-05-07*

#### [MedIE-Instruct: A Comprehensive Instruction Dataset for Medical Information Extraction](https://ieeexplore.ieee.org/document/10884288)

*Zhuoyi Xiang, Xinda Wang, Xiaodong Yan, Deng Zhao, Keyan Ding, Qiang Zhang*

*ICKG 2024*

This paper proposes a resource for medical information extraction, including named entity recognition, relation extraction and event extraction.


*2025-02-23*

#### [Knowledge enhanced representation learning for drug discovery](https://dl.acm.org/doi/10.1609/aaai.v38i9.28924)

*Thanh Lam Hoang, Marco Luca Sbodio, Marcos Martinez Galindo, Mykhaylo Zayats, Raul Fernandez-Diaz, Victor Valls, Gabriele Picco, Cesar Berrospi, Vanessa Lopez*

*AAAI 2024*

This paper proposes a multi-modal knowledge graph for enhancing drug discovery. In particular, each node in the KG is assigned a modal such as text, image, protein sequence, etc., with initial embeddings. Then it utilizes multiple pretrained models to generate and refine node embeddings, with a linear ensemble framework to aggregate the final representations.


*2025-02-14*

#### [Scientific Large Language Models: A Survey on Biological & Chemical Domains](https://arxiv.org/abs/2401.14656)

*Qiang Zhang, Keyan Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Mengyao Zhang, Jinlu Zhang, Jiyu Cui, Renjun Xu, Hongyang Chen, Xiaohui Fan, Huabin Xing, Huajun Chen*

*ACM Computing Surveys, 2025*

This paper reviews existing LLMs on the biological and chemical domains, which are categorized into Textual Scientific / Molecular / Protein / Genomic / Multi-modal Scientific LLMs. For each category, it analyzes existing models, datasets, with evaluation results.


*2025-02-01*

#### [Automated Validating and Fixing of Text-to-SQL Translation with Execution Consistency]()

*Yicun Yang, Zhaoguo Wang, Yu Xia, Zhuoran Wei, Haoran Ding, Ruzica Piskac, Haibo Chen, Jinyang Li*

*SIGMOD 2025*

This paper proposes an approach for automatically detecting and correcting translation errors in Text2SQL datasets. It first introduces a consistency-based semantic concept for Text2SQL—Execution Consistency—as a correctness constraint for Text2SQL translation. Based on this, it leverages SQL equivalence verification techniques to develop SQLDriller as an automated tool for detecting and correcting errors in Text2SQL datasets.


*2025-01-09*

#### [Open-world story generation with structured knowledge enhancement: A comprehensive survey](https://doi.org/10.1016/j.neucom.2023.126792)

*Yuxin Wang, Jieru Lin, Zhiwei Yu, Wei Hu, Börje F. Karlsson*

*Neurocomputing 559*

Incorporating structured knowledge can enhance the logical coherence among stories, achieve better knowledge grounding, and alleviate over-generalization and repetition problems. This paper reviews this field by: (i) presenting a taxonomy regarding how existing methods integrate structured knowledge into story generation; (ii) summarizing existing story corpora, structured knowledge datasets, and evaluation metrics.


*2024-12-25*

#### [Editing Conceptual Knowledge for Large Language Models](https://aclanthology.org/2024.findings-emnlp.40/)

*Xiaohan Wang, Shengyu Mao, Shumin Deng, Yunzhi Yao, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen, Ningyu Zhang*

*EMNLP 2024 Findings*

This paper investigates editing conceptual knowledge for LLMs, by constructing a benchmark dataset and establishing several evaluation metrics.


*2024-11-25*

#### [COKE: A Cognitive Knowledge Graph for Machine Theory of Mind](https://arxiv.org/abs/2305.05390)

*Jincenzi Wu, Zhuang Chen, Jiawen Deng, Sahand Sabour, Helen Meng, Minlie Huang*

*ACL 2024*

This paper proposes a cognitive knowledge graph for enhancing large models to understand human's social cognition. Specifically, it is a set of cognitive chains containing nodes representing (1) situations denote the social circumstances; (2) clues denote the trigger factors; (3) thoughts denote the mental activities; (4) actions denote the behavioral responses; and (5) emotions denote the affective responses. By combining the proposed dataset with LLaMA-2, it also proposes a cognitive LLM for predicting the chain as situation -> clue -> thought -> (action + emotion) that simulates the human's cognitive process.


*2024-11-16*

#### [STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases](https://arxiv.org/abs/2404.13207)

*Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis N. Ioannidis, Karthik Subbian, James Zou, Jure Leskovec*

*NeurIPS 2024*

This paper presents a large-scale Semi-structure retrieval benchmark on Textual and Relational Knowledge Bases, where the key technical challenge is how to accurately simulate user queries on SKBs. This difficulty fundamentally arises from the interdependence of textual and relational information. In this paper, the SKB consists of a entity-relation KB, and a collection of free-text documents where each document is associated with a node in the KB. The process to generate synthetic queries aims to entangle relational and textual information during synthesis and disentangle them during answer filtering.


*2024-09-22*

#### [MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions](https://arxiv.org/abs/2305.14795)

*Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, Danqi Chen*

*EMNLP 2023*

To evaluate the LLMs with injected facts updated by changing model weights, this paper presents a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. Besides, it proposes a simple memory-based approach that stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts, which demonstrates superior performance and scales well to large models.


*2024-07-31*

#### [A Large Scale Test Corpus for Semantic Table Search](https://dl.acm.org/doi/abs/10.1145/3626772.3657877)

*Aristotelis Leventidis, Martin Pekár Christensen, Matteo Lissandrini, Laura Di Rocco, Katja Hose, Renée J. Miller*

*SIGIR 2024*

This paper introduces a dataset for query-by-example (i.e., the query itself is a table) semantic table search, which consists of two snapshots of the large-scale Wikipedia tables collection from 2013 and 2019 and two important additions: (1) a page and topic aware ground truth relevance judgment and (2) a large-scale DBpedia entity linking annotation.


*2024-07-10*

#### [Entity linking for English and other languages: a survey](https://doi.org/10.1007/s10115-023-02059-2)

*Imane Guellil, Antonio Garcia-Dominguez, Peter R. Lewis, Shakeel Hussain, Geoffrey Smith*

*Knowledge and Information Systems 2024*

This paper reviews the research literature on named entity linking, including named entity recognition and disambiguation. It also describes 56 resources including 25 tools and 31 corpora, and presents a set of open issues related to entity linking.


*2024-07-08*

#### [Can AI Beat Undergraduates in Entry-level Java Assignments? Benchmarking Large Language Models on JavaBench](https://arxiv.org/abs/2406.12902)

*Jialun Cao, Zhiyong Chen, Jiarong Wu, Shing-chi Cheung, Chang Xu*

*Arxiv 2024*

This paper propose a project-level Java benchmark that exercises OOP features for evaluating LLMs' code generation capabilities. The experimental results show that: (1) Regarding project-level Java programming, LLMs are far behind undergraduate students; (2) Using method signature as prompt context may strike an ideal balance for project-level code generation.


*2024-06-01*

#### [A Benchmark for Learning to Translate a New Language from One Grammar Book](https://openreview.net/forum?id=tbVWug9f2h)

*Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, Luke Melas-Kyriazi*

*ICLR 2024*

To evaluate the ability of LLMs to adapt to genuinely new tasks, given there are rarely unseen information beyond the internet-scale training sets, this paper explores low-resource languages as a scarcity of web data by presenting a machine translation benchmark based on several hundred pages of field linguistics reference materials. The results demonstrate that baselines using current LLMs are promising but fall short of human performance, which motivates further improvement on machine translation.


*2024-05-18*

#### [Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking](https://papers.nips.cc/paper_files/paper/2023/hash/0be50b4590f1c5fdf4c8feddd63c4f67-Abstract-Datasets_and_Benchmarks.html)

*Juanhui Li, Harry Shomer, Haitao Mao, Shenglai Zeng, Yao Ma, Neil Shah, Jiliang Tang, Dawei Yin*

*NeurIPS 2023 - Datasets and Benchmarks Track*

This paper firstly conducts a comparison across prominent GNN models and datasets, utilizing the same dataset and hyperparameter search settings. Then it creates a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations.


*2024-05-13*

#### [Synthcity: a benchmark framework for diverse use cases of tabular synthetic data](https://papers.nips.cc/paper_files/paper/2023/hash/09723c9f291f6056fd1885081859c186-Abstract-Datasets_and_Benchmarks.html)

*Zhaozhi Qian, Rob Davis, Mihaela van der Schaar*

*NeurIPS 2023*

This work proposes Synthcity, an open-source Python library for one-click benchmarking of synthetic data generators across data modalities and use cases. It provides evaluation metrics for assessing dataset fidelity, privacy, and utility. With a wide array of generators and customizable architectures, users can perform consistent comparisons with existing models, gaining insights into performance improvements.


*2024-03-14*

#### [DyVal: Graph-informed Dynamic Evaluation of Large Language Models](https://openreview.net/forum?id=gjfOL9z5Xr)

*Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie*

*ICLR 2024 Spotlight*

This paper introduces DyVal, an evaluation framework for LLMs based on the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems.


*2024-03-01*

#### [What's In My Big Data?](https://openreview.net/forum?id=RvfPnOkPV4)

*Yanai Elazar, Akshita Bhagia, Ian Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hannaneh Hajishirzi, Noah A. Smith, Jesse Dodge*

*ICLR 2024 Spotlight*

This paper introduces a set of toolkits for analyzing corpora used to pretrain language models, and illustrates how the toolkits can be used to uncover various (but mostly trivial statistical) properties of widely used corpora, such as the amount of duplicate content. The findings also suggest some flaws of existing corpora.


*2024-02-12*

#### [LUBM: A benchmark for OWL knowledge base systems](https://www.sciencedirect.com/science/article/pii/S1570826805000132?via%3Dihub)

*Yuanbo Guo, Zhengxiang Pan, Jeff Heflin*

*Journal of Web Semantics 2005*

This paper proposes the Lehigh University Benchmark (LUBM), which features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, 14 extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms.


*2023-12-23*

#### [SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization](https://aclanthology.org/2023.emnlp-main.799/)

*Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, Yejin Choi*

*EMNLP 2023 Outstanding*

This paper presents SODA as the first publicly available, million-scale high-quality social dialogue dataset. Besides, it trains COSMO as a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna).


*2023-11-22*

#### [Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies](https://doi.org/10.1162/tacl_a_00370)

*Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant*

*TACL 2021*

This paper proposes a (textual) question answering benchmarking dataset for evaluating multi-hop reasoning of the system. Specifically, the questions in this dataset do not explicitly mention the required reasoning steps but implicitly contains the requirement for reasoning. Meanwhile, it provides the decomposition of reasoning steps with Wikipedia paragraphs that contain the answers to each reasoning step.


*2023-10-13*

#### [Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic](https://proceedings.mlr.press/v202/morishita23a.html)

*Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa*

*ICML 2023*

This paper builds a synthetic corpus consisting of deductive reasoning examples for training language models. Specifically, the examples originate from (some simple) FOL deductive rules. The rules are converted to proof trees. Factual distractors are generated based on the proof trees. Then NL templates are used to convert rules and distractors to NL sentences as contexts. Finally, examples are assembled, with each of them containing a set of facts, some hypothesis, a proof pipeline and the label (proved or not).


*2023-08-25*

#### [Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness](https://openreview.net/pdf?id=Wc5bmZZU9cy)

*Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, Steve Ash, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ng, Bing Xiang*

*ICLR 2023*

This paper proposes a benchmark for evaluating text-to-sql robustness. Specifically, it designs 17 kinds of perturbations including database, natural language question, and SQL perturbation test sets to measure the robustness from different angles. This benchmark would be helpful for diagnosing weakness of existing methods.


*2023-08-22*

#### [STREET: A Multi-Task Structured Reasoning and Explanation Benchmark](https://openreview.net/pdf?id=1C_kSW1-k0)

*Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, Zhiheng Huang, William Yang Wang, George Karypis, Bing Xiang, Dan Roth*

*ICLR 2023*

This paper proposes a benchmarking dataset for multi-task and multi-domain natural language reasoning and explanation. The answers in the dataset contain annotated or generated reasoning graphs, where the models are expected to produce step-by-step structured explanations describing how premises in the question are used for making intermediate conclusions that can prove the correctness of a certain answer.


*2023-08-06*

#### [WikiWhy: Answering and Explaining Cause-and-Effect Questions](https://openreview.net/pdf?id=vaxnu-Utr4l)

*Matthew Ho, Aditya Sharma, Justin Chang, Michael Saxon, Sharon Levy, Yujie Lu, William Yang Wang*

*ICLR 2023*

This paper proposes a QA dataset named WikiWhy, containing explanations of "why the answer is true" in natural language. It consists of over 9,000 “why” question-answer-rationale triples, grounded on Wikipedia facts across a diverse set of topics. Each rationale is a set of supporting statements connecting the question to the answer.


*2023-07-11*

#### [MLN4KB: an efficient Markov logic network engine for large-scale knowledge bases and structured logic rules](https://dl.acm.org/doi/10.1145/3543507.3583248)

*Huang Fang, Yang Liu, Yunfeng Cai, Mingming Sun*

*WWW 2023*

Motivated by existing MLN models generally have the inefficient problem, this paper focuses on a certain class of first-order logic rules that are regarded as sufficiently expressive, and develops a highly efficient MLN inference engine called MLN4KB. It is implemented as a Julia package that supports both maximum a posteriori (MAP) inference and learning the weights of rules.


*2023-06-21*

#### [Revisiting Inferential Benchmarks for Knowledge Graph Completion](https://arxiv.org/pdf/2306.04814.pdf)

*Shuwen Liu, Bernardo Cuenca Grau, Ian Horrocks, Egor V. Kostylev*

*KR 2023*

This paper proposes a benchmark for KGC satisfying that: there is a set of logical rules so that the missing facts are the results of the rules’ application; the training set includes both premises matching rule antecedents and the corresponding conclusions; the test set consists of the results of applying the rules to the training set; the negative examples are designed to discourage the models from learning rules not entailed by the rule set.


*2023-05-22*

#### [FactKG: Fact Verification via Reasoning on Knowledge Graphs](https://arxiv.org/pdf/2305.06590.pdf)

*Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, Edward Choi*

*ACL 2023*

This paper proposes a dataset named FactKG for fact verification by reasoning over knowledge graphs. It consists of 5 kinds of claims/reasoning, namely, one-hop, conjunction, existence, multi-hop and negation. The facts are extracted from DBpedia, and the claims are generated based on WebNLG.


*2023-03-29*

#### [Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values](https://dl.acm.org/doi/10.1145/3534678.3539074)

*Zijie J. Wang, Alex Kale, Harsha Nori, Peter Stella, Mark E. Nunnally, Duen Horng Chau, Mihaela Vorvoreanu, Jennifer Wortman Vaughan, Rich Caruana*

*KDD 2022*

This paper introduces an interactive system which allows domain experts to manually edit each prediction function within a Generalized Additive Model (GAM), in order to modify the prediction results ("to fix the problematic parts").


*2023-03-26*

#### [GraphWorld: Fake Graphs Bring Real Insights for GNNs](https://doi.org/10.1145/3534678.3539203)

*John Palowitch, Anton Tsitsulin, Brandon Mayer, Bryan Perozzi*

*KDD 2022*

This paper introduces GraphWorld, a system for benchmarking GNN models on an arbitrarily-large population of synthetic graphs for any conceivable GNN task. It allows a user to efficiently generate a world with millions of statistically diverse datasets. The user has fine-grained control over graph generator parameters, and can benchmark arbitrary GNN models with built-in hyperparameter tuning.


*2023-02-02*

#### [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://knowledge-nlp.github.io/aaai2023/papers/004-ScienceQA-oral.pdf)

*Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan*

*KnowledgeNLP-AAAI 2023*

This paper proposes a benchmarking dataset named ScienceQA for multimodal question answering. It consists about 21k multimodal multiple choice questions of scientific topics, as well as annotations of their answers with corresponding lectures and explanations (which simulate a chain of thoughts, CoT). This paper also tries to design models for generating lectures and explanations. The results demonstrate the utility of CoT in language models as it improves the question answering performance of SOTA models.


*2023-02-01*

#### [Knowledge Retrieval Over Public and Private Data](https://knowledge-nlp.github.io/aaai2023/papers/003-PQA-oral.pdf)

*Simran Arora, Patrick Lewis, Angela Fan, Jacob D Kahn, Christopher Re*

*KnowledgeNLP-AAAI 2023*

This paper studies the task of multi-hop retrieval for question answering over public and private corpora. It firstly formulates the task as a multi-hop (ordered) sequence for retrieval over several public and private corpora, and requires the visiting order to strictly include public corpora before private ones. In this way, the leakage of private data can be avoided. Then it also proposes a public-private benchmarking dataset for evaluating multi-hop retrieval-based QA methods.


*2023-01-31*

#### [ComFact: A Benchmark for Linking Contextual Commonsense Knowledge](https://knowledge-nlp.github.io/aaai2023/papers/002-ComFact-oral.pdf)

*Silin Gao, Jena D Hwang, Saya Kanno, Hiromi Wakaki, Yuki Mitsufuji, Antoine Bosselut*

*KnowledgeNLP-AAAI 2023*

This paper proposes a benchmark dataset for contextual commonsense knowledge linking, an example application of which is to find relevant pieces of fact in the KG for a dialog or story (contexts). To build this benchmark dataset, the authors firstly apply string matching and SentenceBERT to generate and filter candidate facts. Then the relevance of these facts are judged by crowdsourcing workers. In the experiments, several popular fact linking methods are evaluated over the dataset, showing that their performances are still far behind human beings. 


*2023-01-19*

#### [Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand](https://aclanthology.org/2022.naacl-main.259/)

*Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander R. Fabbri, Yejin Choi, Noah A. Smith*

*NAACL 2022*

Unlike typical leaderboards which generally compare language models based on BLEU/ROUGE, this paper proposes a bidimensional leaderboard accepting both generation models and evaluation metrics. The leaderboard automatically creates an ensemble metric based on global analysis of the models. It will also rank and compare the metrics based on the correlation with human judgements. 


*2023-01-07*

#### [μ KG: A Library for Multi-source Knowledge Graph Embeddings and Applications](https://link.springer.com/chapter/10.1007/978-3-031-19433-7_35)

*Xindi Luo, Zequn Sun, Wei Hu*

*ISWC 2022*

This resource paper provides an open-source Python library consisting of 26 popular knowledge graph embedding models and 16 benchmark datasets. 


*2022-12-22*

#### [WDV: A Broad Data Verbalisation Dataset Built from Wikidata](https://link.springer.com/chapter/10.1007/978-3-031-19433-7_32)

*Gabriel Amaral, Odinaldo Rodrigues, Elena Simperl*

*ISWC 2023*

This resource paper proposes a dataset consists of over 7.6k Wikidata entries and their verbalized textual version. The dataset is made up from crowdsourcing annotations. Each entry contains textual descriptions of the subject, predicate and object in Wikidata, respectively.  


*2022-11-28*

#### [HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data](https://doi.org/10.18653/v1/2020.findings-emnlp.91)

*Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Yang Wang*

*EMNLP Findings 2020*

This paper proposes a question answering dataset containing both a Wikipedia table and a set of passages as resources. It begins with selecting Wikipedia tables which have hyperlinked cells. Then it retrieves the Wikipedia pages of each cell and collects the first sentences as supporting textual data. The questions are collected using AMT and post-processed to avoid bias. 


*2022-11-27*

#### [FinQA: A Dataset of Numerical Reasoning over Financial Data](https://doi.org/10.18653/v1/2021.emnlp-main.300)

*Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R. Routledge, William Yang Wang*

*EMNLP 2021*

This paper proposes a dataset named FinQA containing question-answer pairs with annotated reasoning explanations based on financial reports. By implementing existing QA methods as baseline, the experimental results suggest existing pre-trained language models still fall far short than humans in acquiring finance knowledge and conducting multi-step numerical reasoning. 
