








*2023-09-12*

#### [PAC Prediction Sets for Large Language Models of Code](https://proceedings.mlr.press/v202/khakhar23a.html)

*Adam Khakhar, Stephen Mell, Osbert Bastani*

*ICML 2023*

This paper investigates the task of constructing PAC sets for code generation. It introduces a notion of partial programs as prediction sets for code generation, and proposes an algorithm for construction PAC prediction sets under this setting.


*2023-09-11*

#### [Trainability, Expressivity and Interpretability in Gated Neural ODEs](https://proceedings.mlr.press/v202/kim23b.html)

*Timothy Doyeon Kim, Tankut Can, Kamesh Krishnamurthy*

*ICML 2023*

Neural ordinary differential equations (nODEs) is a class of dynamical models with a velocity field parametrized by a deep neural network, which can potentially implement more complex computations in lower dimensions than classical RNNs. This paper introduces a gating interaction for nODEs, designs a new measure of expressivity related to the network capacity, and demonstrates its superiority.


*2023-09-10*

#### [On the Relationship Between Explanation and Prediction: A Causal View](https://proceedings.mlr.press/v202/karimi23a.html)

*Amir-Hossein Karimi, Krikamol Muandet, Simon Kornblith, Bernhard Schölkopf, Been Kim*

*ICML 2023*

This paper investigates the relationship between the model prediction (Y) and the explanation (E) with a casual influence view. It measures the treatment effect when intervening on their casual ancestors, i.e., the hyperparameters and the inputs used to generate saliency-based Es or Ys.


*2023-09-09*

#### [Leveraging Proxy of Training Data for Test-Time Adaptation](https://proceedings.mlr.press/v202/kang23a.html)

*Juwon Kang, Nayeong Kim, Donghyeon Kwon, Jungseul Ok, Suha Kwak*

*ICML 2023*

Test-time adaptation (TTA) is the task of adapting a trained model to an arbitrary test domain using unlabeled input data on-the-fly during testing. This paper proposes two lightweight proxies of the training data and a TTA method that fully exploits them.


*2023-09-03*

#### [Learning Unnormalized Statistical Models via Compositional Optimization](https://proceedings.mlr.press/v202/jiang23g.html)

*Wei Jiang, Jiayu Qin, Lingyu Wu, Changyou Chen, Tianbao Yang, Lijun Zhang*

*ICML 2023*

Existing noise contrastive estimation methods for learning unnormalized statistical models suffer from bad performance and slow convergence. This paper proposes a direct approach for optimizing negative log-likelyhood of models by converting it to a stochastic compositional optimization (SCO) problem.


*2023-08-31*

#### [On the Impact of Knowledge Distillation for Model Interpretability](https://proceedings.mlr.press/v202/han23b.html)

*Hyeongrok Han, Siwon Kim, Hyun-Soo Choi, Sungroh Yoon*

*ICML 2023*

This paper argues that knowledge distillation of models can not only enhance the performance but also improve the interpretability. It measures the model's interpretability by the number of concept detectors introduced in network dissection, and attributes the improvement of interpretability to the class similarity transferred from the teacher model to the student model.


*2023-08-28*

#### [Do We Really Need Complicated Model Architectures For Temporal Networks?](https://openreview.net/pdf?id=ayPPc0SyLv1)

*Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang Tong, Mehrdad Mahdavi*

*ICLR 2023*

This paper proposes a simple but effective network architecture for temporal graph learning. Specifically, it only includes a link encoder based on MLP, a node encoder of mean-pooling of neighbors, and an MLP-based link classifier for link predictions. It demonstrates good performance in practice.


*2023-08-27*

#### [On the duality between contrastive and non-contrastive self-supervised learning](https://openreview.net/pdf?id=kDEL91Dufpa)

*Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, Yann LeCun*

*ICLR 2023*

This paper investigates the theoretical and empirical similarities of the so-called contrastive and non-constrastive learning methods, especially the covariance regularization-based non-contrastive methods. It proposes to unify both kinds of approaches with a pair of contrastive and non-contrastive criteria based on F-norm and embedding normalizations.


*2023-08-26*

#### [In-context Reinforcement Learning with Algorithm Distillation](https://openreview.net/pdf?id=hy0a5MMPUv)

*Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan A. Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, Volodymyr Mnih*

*ICLR 2023*

This paper proposes a method for simulating reinforcement learning using in-context learning by regarding it as a sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context.


*2023-08-23*

#### [No Reason for No Supervision: Improved Generalization in Supervised Models](https://openreview.net/pdf?id=3Y5Uhf5KgGK)

*Mert Bülent Sariyildiz, Yannis Kalantidis, Karteek Alahari, Diane Larlus*

*ICLR 2023*

This paper proposes a supervised training setup that incorporates multi-crop data augmentation and an expendable projector that can produce models with favorable performance both on the training task and transfer tasks.


*2023-08-19*

#### [Relational Attention: Generalizing Transformers for Graph-Structured Tasks](https://openreview.net/pdf?id=cFuMmbWiN6)

*Cameron Diao, Ricky Loynd*

*ICLR 2023*

This paper proposes an adapted version of transformer blocks for relational graphs, named Relational Transformer. In addition to accepting node vectors representing entity features (as do all transformers), RT also accepts edge vectors representing relation features, which may include edge-presence flags from an adjacency matrix. (But overall it operators on a fully-connected graph, unconstrained by any input adjacency matrix.)


*2023-08-18*

#### [Effects of Graph Convolutions in Multi-layer Networks](https://openreview.net/pdf?id=P-73JPgRs0R)

*Aseem Baranwal, Kimon Fountoulakis, Aukosh Jagannath*

*ICLR 2023*

This paper theoretically explores the effects of graph convolution in multi-layer networks with a node classification problem. It shows that a single graph convolution enables a multi-layer network to classify the nodes with a larger threshold (by a factor) of the distance between the means of node features. In a graph with a higher density, two graph convolutions will further improve the factor.


*2023-08-17*

#### [Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization](https://openreview.net/pdf?id=8aHzds2uUyB)

*Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi*

*ICLR 2023*

This paper proposes to solve NLP tasks in a reinforcement learning view. It uses an open-sourced library for optimizing language generators with RL, and presents a benchmark consisting of 6 natural language generation tasks with reward functions from human preference. Besides, it also provides an RL algorithm named NLPO that learns to effectively reduce the combinatorial action space in language generation.


*2023-08-14*

#### [Sparse Mixture-of-Experts are Domain Generalizable Learners](https://openreview.net/pdf?id=RecZ9nB9Q4)

*Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, Ziwei Liu*

*ICLR 2023*

To address the problem of domain generalization, this paper proposes a model built upon vision transformers, in which the network's robustness to distribution shifts is characterized by the architecture's alignment with the correlations in the dataset.


*2023-08-12*

#### [Learning with Logical Constraints but without Shortcut Satisfaction](https://openreview.net/pdf?id=M2unceRvqhh)

*Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, Jian Lü*

*ICLR 2023*

Noticing that existing methods with loss function including logical constraints may be bypassed with shortcuts thus not fully exploiting intrinsic knowledge, this paper designs a new loss function for logical constraints. It introduces an additional random variable for the logical constraint indicating its satisfaction degree, and formulates it as a distributional loss which is compatible with the neural network’s original training loss under a variational framework.


*2023-08-11*

#### [A Survey on the Explainability of Supervised Machine Learning](https://jair.org/index.php/jair/article/view/12228)

*Nadia Burkart, Marco F. Huber*

*JAIR 2021*

This paper discusses the essential definitions, an overview of different principles and methodologies of explainable supervised machine learning.


*2023-08-07*

#### [What learning algorithm is in-context learning? Investigations with linear models](https://openreview.net/pdf?id=0g0X4H8yN4I)

*Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou*

*ICLR 2023*

This paper investigates the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. The hypothesis is evaluated with the problem of linear regression, and demonstrated with several source of evidence.


*2023-08-05*

#### [Encoding Recurrence into Transformers](https://openreview.net/pdf?id=7YfHla7IxBJ)

*Feiqing Huang, Kexin Lu, Yuxi Cai, Zhen Qin, Yanwen Fang, Guangjian Tian, Guodong Li*

*ICLR 2023*

This paper proposes to equivalently replace a RNN layer with a set of simple RNNs, and further by a multi-head self-attention block. It further proposes a new module named Self-Attention with Recurrence, which can incorporate the recurrent dynamics into a transformer.


*2023-07-03*

#### [Normalizing Flow-based Neural Process for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2304.08183)

*Linhao Luo, Reza Haffari, Yuan Fang Li, Shirui Pan*

*SIGIR 2023*

Neural Processes (NPs) combine the stochastic process and neural networks to define a distribution over prediction functions with limited observed data. Normalizing flows (NFs) [36] employ a sequence of bijective mapping functions to transform a simple distribution into a complex target distribution. This paper applies the NPs and normalized flows in an encoder-decoder model for KGC.


*2023-05-21*

#### [KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment](https://arxiv.org/pdf/2305.06535.pdf)

*Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, Hongzhi Yin*

*ACL 2023*

Machine unlearning refers to the ability of the learned model to forget information about specific training data as if they never existed in the training set. This paper follows the idea of approximate unlearning, whose goal is to forget the data to be forgotten while maintaining the performance. To achieve this, it optimizes the model to have similar behaviors on the data to be forgotten as unseen data, while maintaining the performance on the rest of data.
