



*2023-08-05*

#### [Encoding Recurrence into Transformers](https://openreview.net/pdf?id=7YfHla7IxBJ)

*Feiqing Huang, Kexin Lu, Yuxi Cai, Zhen Qin, Yanwen Fang, Guangjian Tian, Guodong Li*

*ICLR 2023*

This paper proposes to equivalently replace a RNN layer with a set of simple RNNs, and further by a multi-head self-attention block. It further proposes a new module named Self-Attention with Recurrence, which can incorporate the recurrent dynamics into a transformer.


*2023-07-03*

#### [Normalizing Flow-based Neural Process for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2304.08183)

*Linhao Luo, Reza Haffari, Yuan Fang Li, Shirui Pan*

*SIGIR 2023*

Neural Processes (NPs) combine the stochastic process and neural networks to define a distribution over prediction functions with limited observed data. Normalizing flows (NFs) [36] employ a sequence of bijective mapping functions to transform a simple distribution into a complex target distribution. This paper applies the NPs and normalized flows in an encoder-decoder model for KGC.


*2023-05-21*

#### [KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment](https://arxiv.org/pdf/2305.06535.pdf)

*Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, Hongzhi Yin*

*ACL 2023*

Machine unlearning refers to the ability of the learned model to forget information about specific training data as if they never existed in the training set. This paper follows the idea of approximate unlearning, whose goal is to forget the data to be forgotten while maintaining the performance. To achieve this, it optimizes the model to have similar behaviors on the data to be forgotten as unseen data, while maintaining the performance on the rest of data.
