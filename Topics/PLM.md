






*2022-10-29*

#### [EventBERT: A Pre-Trained Model for Event Correlation Reasoning](https://doi.org/10.1145/3485447.3511928)

*Yucheng Zhou, Xiubo Geng, Tao Shen, Guodong Long, Daxin Jiang*

*TheWebConf 2022*

This paper proposes a pre-trained model to conduct event correlation reasoning. It firstly collects a set of training examples (natural language paragraphs and events) from a large book corpus. Then it proposes three self-supervised event-based and correlation-based learning objectives to pre-train the model, including correlation-based relation ranking, contradiction event tagging and discourse relation ranking. The former two train the model to distinguish the correct event against negative ones, while the latter helps the model identify subtle difference among discourse relations. 


*2022-10-23*

#### [Enhancing Knowledge Bases with Quantity Facts](https://doi.org/10.1145/3485447.3511932)

*Vinh Thinh Ho, Daria Stepanova, Dragan Milchevski, Jannik Str√∂tgen, Gerhard Weikum* (MPI)

*TheWebConf 2022*

This paper proposes a recall-oriented knowledge base augmentation method named QL, to add missing quantity facts into the existing KB. It extracts facts from external text corpus, and divides them into high-confidence and low-confidence groups. Then it iteratively consolidates the facts using distribution-based denoising method and expends the query with more equivalent properties. In the experiments, QL is compared with other question answering methods including RoBERTa, QSearch and GPT-3 on precision, recall and novelty. 


*2022-10-23*

#### [Unified Question Generation with Continual Lifelong Learning](https://dl.acm.org/doi/10.1145/3485447.3511930)

*Wei Yuan, Hongzhi Yin, Tieke He, Tong Chen, Qiufeng Wang, Lizhen Cui:*

*TheWebConf 2022*

This paper proposes a unified model for natural language question generation (QG) tasks. It is based on *T5* for natural language generation. Firstly, it unifies four formats of QG (i.e., answer-extraction, answer-abstraction, multi-choice and boolean QG) by concatenating their different components (i.e., the answer, passage, distractor, etc. ) as input, respectively. Then it applies life-long learning by keeping and re-playing difficult examples with similarity regularization to reduce the negative effect of forgetting history. 


*2022-10-21*

#### [Ontology-enhanced Prompt-tuning for Few-shot Learning](https://doi.org/10.1145/3485447.3511921)

*Hongbin Ye, Ningyu Zhang, Shumin Deng, Xiang Chen, Hui Chen, Feiyu Xiong, Xi Chen, Huajun Chen*

*TheWebConf 2022*

This paper proposes to use ontology information to enhance prompt-tuning in few-shot learning tasks. It evaluates three tasks including relation extraction, event extraction and knowledge graph completion in this paper. The ontology information of target entities used in this paper is extracted from the knowledge graph, and used as plain texts as auxiliary prompts for PLM. 
