









*2023-10-22*

#### [Bag of Tricks for Training Data Extraction from Language Models](https://proceedings.mlr.press/v202/yu23c.html)

*Weichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min Lin, Shuicheng Yan*

*ICML 2023*

This paper investigates the effectiveness of tricks of training data extraction for language models. Specifically, it measures the precision, recall and Hamming distance increment brought by different tricks including sampling, probability distribution adjustment, exposure bias reduction, look ahead, sentence-level criteria and token-level criteria. The empirical results show that several of these tricks improve the results significantly, while interactions between different tricks are more subtle than expected. Besides, the commonly used versatile methods for general text generation are not always effective for extraction tasks.


*2023-10-17*

#### [Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models](https://proceedings.mlr.press/v202/shao23a.html)

*Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen*

*ICML 2023*

This paper proposes a prompt generation method, which is based on a few handcrafted examples. It generates more examples and selects effective demonstrations to improve reasoning. Specifically, it applies an alternating forward-backward process, in which the backward process involves generating a question for the given reasoning chain, and the forward process produces a better reasoning chain for the generated question.


*2023-10-15*

#### [Can Large Language Models Reason about Program Invariants?](https://proceedings.mlr.press/v202/pei23a.html)

*Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, Pengcheng Yin*

*ICML 2023*

This paper investigates the application of using LLMs to identify program invariants. Typical invariant prediction approaches usually rely on dynamic analysis which requires multiple traces collected from executing the codes. However, this paper shows that LMs trained on source codes and fine-tuned for invariant generation are able to perform static invariant prediction with a scratch-pad approach.


*2023-09-25*

#### [The Unreasonable Effectiveness of Few-shot Learning for Machine Translation](https://proceedings.mlr.press/v202/garcia23a.html)

*Xavier Garcia, Yamini Bansal, Colin Cherry, George F. Foster, Maxim Krikun, Melvin Johnson, Orhan Firat*

*ICML 2023*

This paper demonstrates the few-shot training with high-quality data can produce translation models with relatively high performance. It highlights the quality of few-shot demonstrations can heavily determine the quality of translations generated by the model. Besides, the few-shot paradigm also provides a way to control certain variabilities of machine translation such as regional varieties.


*2023-09-23*

#### [SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot](https://proceedings.mlr.press/v202/frantar23a.html)

*Elias Frantar, Dan Alistarh*

*ICML 2023*

This paper proposes to prune the GPT model family to at least 50% sparsity of parameters in one-shot without retraining. It is based on a set of techniques including post-training pruning, layer-wise pruning, mask selection and weight reconstruction. It further proposes an algorithm for the pruning process.


*2023-09-14*

#### [A Watermark for Large Language Models](https://proceedings.mlr.press/v202/kirchenbauer23a.html)

*John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein*

*ICML 2023*

This paper proposes a watermarking method for LLMs based on the selection of synonyms when there are multiple choices for generating texts. A watermark is a hidden pattern in text that is imperceptible to humans, while making the text algorithmically identifiable as synthetic. The proposed watermarking method uses a list of "favored" words in text generation, which is invisible to humans while can be captured by computing entropy and probability distribution among words.


*2023-09-08*

#### [Large Language Models Struggle to Learn Long-Tail Knowledge](https://proceedings.mlr.press/v202/kandpal23a.html)

*Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel*

*ICML 2023*

This paper explores the relationship between the knowledge learned by an LLM and the information in its pretraining data. Specifically, it studies how an LLM’s ability to answer a question relates to how many documents associated with that question were seen during pretraining. The results indicate a strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pretraining corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). It also concludes that model scaling and retrieval-augmentation help better capture the knowledge that rarely appears in the pretraining data.


*2023-09-05*

#### [Editing Large Language Models: Problems, Methods, and Opportunities](https://arxiv.org/abs/2305.13172)

*Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang*

*Arxiv 2023*

This paper summarizes existing methods for editing LLMs, including parameter-preserving and -modifying methods, compares their performance, and proposes a new dataset for evaluating them, especially in terms of generalization and efficiency.


*2023-09-04*

#### [Automatically Auditing Large Language Models via Discrete Optimization](https://proceedings.mlr.press/v202/jones23a.html)

*Erik Jones, Anca D. Dragan, Aditi Raghunathan, Jacob Steinhardt*

*ICML 2023*

This paper formulates auditing LLMs as a discrete optimizing problem, and proposes an algorithm for automatically searching for input-output pairs that match a target behavior (e.g., toxic output). To improve the search efficiency, it further decomposes the optimizing target as a linear approximation and an autoregressive part.


*2023-09-02*

#### [Exploring the Benefits of Training Expert Language Models over Instruction Tuning](https://proceedings.mlr.press/v202/jang23a.html)

*Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, Minjoon Seo*

*ICML 2023*

This paper argues that instead of using large amount of multi-task data for prompt-tuning a general LM, fine-tuning an "expert" LM on a single task provides better performance. Following this, it also proposes separately training LMs on a single task brings benefits include (1) avoiding negative task transfer, (2) being able to continually learn new tasks without having to re-train on previous tasks to avoid catastrophic forgetting, and (3) showing compositional capabilities when merging individual experts together.


*2023-08-24*

#### [Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought](https://openreview.net/pdf?id=qFVVBzXxR2V)

*Abulhair Saparov, He He*

*ICLR 2023*

This paper proposes a synthetic question-answering dataset for evaluating the reasoning capabilities of LLMs. To generate each example, it firstly samples an ontology as a tree, and secondly produces a symbolic proof from the ontology. Thirdly, it converts the ontology into a natural language context, and finally, the proof is converted into a natural language question, chain-of-thought, and an answer label (True or False).


*2023-08-21*

#### [Neuro-Symbolic Procedural Planning with Commonsense Prompting](https://openreview.net/pdf?id=iOc57X9KM54)

*Yujie Lu, Weixi Feng, Wanrong Zhu, Wenda Xu, Xin Eric Wang, Miguel P. Eckstein, William Yang Wang*

*ICLR 2023*

Procedural planning aims to implement complex high-level goals by decomposition into simpler low-level steps. This paper proposes to leverage an external knowledge graph to automatically produce causal prompts for LLMs to conduct procedural planning.


*2023-08-20*

#### [Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies](https://arxiv.org/abs/2308.03188)

*Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang Wang*

*Arxiv 2023*

Self-correction for LLM is to prompt the model to fix problems in its own output. This survey paper categorizes existing methods for self-correction into three kinds, namely, training-time, generation-time and post-hoc correction, as well as their applications.


*2023-08-13*

#### [Ask Me Anything: A simple strategy for prompting language models](https://openreview.net/pdf?id=bhUPJnS2g0X)

*Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel J. Orr, Neel Guha, Kush Bhatia, Ines Chami, Christopher Ré*

*ICLR 2023*

Observed that question-answering (QA) prompts, which encourage open-ended generation (“Who went to the park?”) tend to outperform those that restrict the model outputs (“John went to the park. True or False?”), this paper proposes a simple prompting method named Ask Me Anything (AMA). It recursively uses the LLM to transform task inputs to the effective QA format, generates multiple questions per input, and applies these prompts to collect several noisy votes for the input’s true label.


*2023-08-10*

#### [Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs](https://openreview.net/pdf?id=SMa9EAovKMC)

*Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothée Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, Yuhuai Wu*

*ICLR 2023*

This paper investigates the task of automatic formal proof generation based on informal sketches. The informal drafts are firstly produced either by ordinary human user or LLMs. Then a mapping model is prompted to match the parts of informal sketches to a high-level structure of the formal proof. Finally, an off-the-shelf automated prover is executed to fill-in the gaps of the formal proof.


*2023-08-09*

#### [Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning](https://openreview.net/pdf?id=3Pf3Wg6o-A4)

*Antonia Creswell, Murray Shanahan, Irina Higgins*

*ICLR 2023*

This paper investigates the ability of LLMs to perform multi-step logical reasoning tasks. Motivated by the fact that LLMs usually perform well on single step inference and entailments, this paper further proposes a selection-inference module to guide the model with an interpretable, causal reasoning chain leading to the final answer. In the experiment, it uses pre-trained, frozen language models in a 5-shot generalisation setting with prompt engineering to implement the selection and inference modules.


*2023-07-02*

#### [Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning](https://arxiv.org/abs/2301.13808)

*Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li*

*SIGIR 2023*

This paper proposes to use LLM for decomposing both large tables and complex questions for table-based reasoning for QA. On the one hand, it applies LLM to decompose large tables into small sub-tables while retaining the useful evidences and giving up the redundancy for a given question. On the other hand, it also  uses a LLM to decompose complex questions into chain of steps (as sub-questions), which is more convenient for SQL query generation.


*2023-06-29*

#### [Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?](https://arxiv.org/abs/2302.03495)

*Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon*

*SIGIR 2023*

Systematic reviews are comprehensive reviews of the literature for a highly focused research question. This paper investigates using prompt engineering with ChatGPT to generate boolean queries for systematic reviews. By analyzing the results, it reveals that (1) ChatGPT give good performance with better precision than recall, (2) the type of prompts used has considerable effects on the effectiveness of the queries produced by ChatGPT, (3) guided prompts lead to higher effectiveness than single prompt strategies for both precision and recall, (4) two main issues, i.e., incorrect MeSH terms, and high variability in query effectiveness across multiple requests.


*2023-06-25*

#### [Unifying Large Language Models and Knowledge Graphs: A Roadmap](https://arxiv.org/abs/2306.08302)

*Shirui Pan, Linhao Luo,Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu*

*Arxiv 2021*

This paper discusses three methods as a roadmap for unifying LLMs and KGs including: (1) KG-enhanced LLMs, which incorporates KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs, (2) LLM-augmented KGs, that leverages LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering, (3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge.


*2023-06-24*

#### [WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences](https://arxiv.org/pdf/2306.07906.pdf)

*Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, Jie Tang*

*KDD 2023*

This paper proposes WebGLM, a LLM-based system with Web search abilities for question answering. The system is compared with WebGPT. This paper also proposes systematic criteria for evaluating Web-enhanced QA systems.


*2023-06-01*

#### [Can Language Models Solve Graph Problems in Natural Language?](https://arxiv.org/abs/2305.10037)

*Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov*

*Arxiv 2023*

This paper proposes a benchmark of graph-based reasoning tasks designed for evaluating LLMs. The evaluation results on GPT-3/4 demonstrate that: (1) LLMs do possess preliminary graph reasoning abilities. (2) The benefit of advanced prompting methods diminishes with complex problems. (3) Learning from examples did not happen on complex graph reasoning problems. (4) LLMs are (un)surprisingly brittle to spurious correlations in problem settings.


*2023-05-31*

#### [From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?](https://arxiv.org/pdf/2305.10568.pdf)

*Jordan Coil, Vered Shwartz*

*ACL 2023*

Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). This paper investigates this task by modifying the noun compounds with rare or novel compounds, e.g., chocolate crocodile, and re-evaluates the performance of LLMs (e.g., GPT-3). The result shows that the outputs from GPT-3 often have significant overlap with a large Web corpus, but the parroting strategy is less beneficial for novel noun compounds.


*2023-05-29*

#### [What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning](https://arxiv.org/pdf/2305.09731.pdf)

*Jane Pan, Tianyu Gao, Howard Chen, Danqi Chen*

*ACL 2023*

This paper investigates the in-context learning ability of LLMs by dividing it into task recognition and task learning. It mainly reveals that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL’s performance consistently improves with more demonstrations in context.


*2023-05-25*

#### [Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales](https://arxiv.org/abs/2305.07095)

*Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi, Xiang Ren*

*ACL 2023*

As LLMs being able to generate free-text rationales (i.e., chain-of-thoughts as explanation), this paper investigates the ability of LMs to help humans answer questions based on the generated rationales. To achieve this, it proposes a metric Gen-U to estimate the rationale’s helpfulness in answering similar but unseen instances. It also shows that the LMs can be trained to have better human utility by optimizing Gen-U while retaining similar performance on other tasks.


*2023-04-18*

#### [RRHF: Rank Responses to Align Language Models with Human Feedback without tears](https://arxiv.org/pdf/2304.05302.pdf)

*Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang*

*Arxiv 2023*

InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). To address the problem that PPO is sensitive to parameter and hard to train, this paper proposes another learning method by aligning model response with human feedback through ranking loss.


*2023-04-09*

#### [How well do Large Language Models perform in Arithmetic tasks](https://arxiv.org/pdf/2304.02015.pdf)

*Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang*

*Arxiv 2023*

This paper evaluates the ability of LLMs handling arithmetic tasks, such as Euler equation and decimal calculation. It also analyzes the failed cases of ChatGPT and the effectiveness of different prompts.


*2023-04-06*

#### [Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!](https://arxiv.org/abs/2303.08559)

*Yubo Ma, Yixin Cao, YongChing Hong, Aixin Sun*

*Arxiv 2023*

Although LLMs exhibit attractive in-context learning ability, their performance is sometimes limited due to the constrained input length, and increasing the number of demonstrations cannot significantly improve the performance of LLMs. Therefore, this paper proposes a filter-then-rerank pipeline, which firstly utilizes small LMs (SLMs) to handle the easy cases, then passes the complex part to LLMs as a reranking task. The comparison shows that LLMs perform better than SLMs on complex (i.e., "harder") tasks.


*2023-04-04*

#### [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace](https://arxiv.org/pdf/2303.17580.pdf)

*Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang*

*Arxiv 2023*

This paper proposes to combine ChatGPT with AI models available in HuggingFace to solve complex tasks. Specifically, it uses ChatGPT to conduct task planning when receiving a user request, selects models according to their function descriptions available in HuggingFace, executes each subtask with the selected
AI model, and summarizes the response according to the execution results.

