













*2026-02-20*

#### [The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic](https://arxiv.org/abs/2505.08021)

*Bernardo Cuenca Grau, Eva Feng, Przemysław A. Wałęga*

*AAAI 2026*

[Copied from conclusion and future work.] ``We have introduced families of bounded GNNs, whose expressive power corresponds exactly to well-known modal logics and 2-variable first-order logics. Among others, we have showed that standard aggregate-combine GNNs with bounded aggregation have the same expressive power as the graded modal logic. The correspondence between FO-expressibility and bounding aggregation (and readout) occurs as an interesting phenomenon to study. In particular, we find it interesting to determine for which classes of GNNs classifiers, FO-expressibility is equivalent to expressibility by bounded GNNs.''


*2025-12-17*

#### [Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach](https://arxiv.org/abs/2511.23335)

*Shuqi Liu, Han Wu, Guanzhi Deng, Jianshu Chen, Xiaoyang Wang, Linqi Song*

*Arxiv 2025*

This work employs a 2-layer framework to capture high-level entities and low-level knowledge triples to design a task-agnostic structured knowledge hunter. Specifically, it uses a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. The Knowledge Hunter is a hierarchical encoder-decoder system that searches for relevant knowledge triples from input knowledge graphs or numerical tables. These knowledge triples are then processed through a Template Conversion stage that transforms them into a format that can be readily fed into the language model. Finally, the language model generates text based on the processed knowledge triples.


*2025-12-16*

#### [Beyond Query-Level Comparison: Fine-Grained Reinforcement Learning for Text-to-SQL with Automated Interpretable Critiques](https://arxiv.org/abs/2511.22258)

*Guifeng Wang, Yuanfeng Song, Meng Yang, Tao Zhu, Xiaoming Yin, Xing Chen*

*Arxiv 2025*

To improve the Text-to-SQL performance with RL models, this work introduces a generative judge model for fine-grained, query-specific automatic evaluation without human intervention. It first generates query-specific evaluation rubrics for human-free annotation, linking them to interpretable critiques. Then it integrates densified reward feedback through a "progressive exploration" during the RL training process, which dynamically adjusts the rewards to enhance the model's performance.


*2025-12-01*

#### [Enhancing Logical Expressiveness in Graph Neural Networks via Path-Neighbor Aggregation](https://arxiv.org/abs/2511.07994)

*Han Yu, Xiaojuan Zhao, Aiping Li, Kai Chen, Ziniu Liu, Zhichao Peng*

*Arxiv 2025*

This work proposes Path-Neighbor enhanced GNN, a GNN model with enhanced logical expressive power by aggregating node-neighbor embeddings on the reasoning path. The proposed model not only has stronger expressive power than conditional GNN, but also its (k + 1)-hop logical expressiveness is strictly superior to that of k-hop.


*2025-11-30*

#### [Revisiting Conjunctive Query Entailment for $\mathcal S$](https://arxiv.org/abs/2511.07933)

*Yazmín Ibáñez-García, Jean Christoph Jung, Vincent Michielini, Filip Murlak*

*AAAI 2026*

This work clarifies the complexity of answering unions of conjunctive queries over knowledge bases formulated in the description logic $\mathcal S$, the extension of $\mathcal{ALC}$ with transitive roles. Contrary to existing partial results, it shows the problem is in fact 2ExpTime-complete; hardness already holds in the presence of two transitive roles and for Boolean conjunctive queries. It also shows that the problem remains in coNExpTime when the input query is rooted or is restricted to use at most one transitive role (but may use arbitrarily many non-transitive roles).


*2025-11-13*

#### [Sound Logical Explanations for Mean Aggregation Graph Neural Networks](https://openreview.net/forum?id=7TY89cqLfE)

*Matthew Morris, Ian Horrocks*

*NeurIPS 2025*

(1) Consider mean-aggregated GNNs with non-negative weights, (2) Define a new fragment of FOL to express the equivalent logic.


*2025-11-08*

#### [Explainable Benchmarking through the Lense of Concept Learning](https://arxiv.org/abs/2510.20439)

*Quannian Zhang, Michael Röder, Nikit Srivastava, N'Dah Jean Kouagou, Axel-Cyrille Ngonga Ngomo*

*K-CAP 2025*

The aim of explainable benchmarking is to automatically generate explanations for the system performance over a benchmark. This work introduces an instantiation of this paradigm for KG-based question answering systems, by computing explanations using a concept learning algorithm PruneCEL. It contains 3 steps: (1) generate a KG comprising structured information about the content of the benchmark dataset, (2) split the benchmark’s tasks, e.g., the questions of a QA dataset, into correctly and incorrectly answered tasks, and (3) apply concept learning to determine an expression that separates the two groups from each other. Example explanations are like: the system can answer questions that have xx type as answers.


*2025-10-31*

#### [INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection](https://arxiv.org/abs/2402.03744)

*Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, Jieping Ye*

*ICLR 2024*

This work explores the dense semantic information retained within LLMs' internal states for hallucination detection. It proposes an EigenScore metric to evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency in the dense embedding space. Furthermore, for self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, reducing overconfident generations and potentially benefits the detection of overconfident hallucinations.


*2025-10-02*

#### ["Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://aclanthology.org/N16-3020/)

*Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin*

*NAACL 2016*

This work introduces (1) LIME, an algorithm that can explain the predictions of any classifier by approximating it locally with an interpretable model, and (2) SP-LIME, a method that selects a set of representative explanations via submodular optimization.


*2025-03-17*

#### [SpArX: Sparse Argumentative Explanations for Neural Networks [Technical Report]](https://arxiv.org/abs/2301.09559)

*Hamed Ayoobi, Nico Potyka, Francesca Toni*

*ECAI 2023*

This paper investigates the explainability of MLPs through the lens of quantitative argumentative framework. In particular, it formulates a general MLP with a QAF instance, and identifies the attack/support relationships between neurons to give explanations for one or all the MLP outputs. It does not guarantee fully faithfulness of the explaination to the output, instead, it uses a quantitative value to indicate the faithfulness/unfaithfulness.


*2024-08-13*

#### [SES: Bridging the Gap Between Explainability and Prediction of Graph Neural Networks](https://arxiv.org/abs/2407.11358)

*Zhenhua Huang, Kunhao Li, Shaojie Wang, Zhaohong Jia, Wentao Zhu, Sharad Mehrotra*

*ICDE 2024*

This paper proposes an "explainable" GNN model that comprises two processes: explainable training and enhanced predictive learning. During explainable training, it employs a global mask generator co-trained with a graph encoder and directly produces crucial structure and feature masks, reducing time consumption and providing node feature and subgraph explanations. During enhanced predictive learning, mask-based positive-negative pairs are constructed utilizing the explanations to compute a triplet loss and enhance the node representations by contrastive learning.


*2024-08-12*

#### [Expressivity and Generalization: Fragment-Biases for Molecular GNNs](https://arxiv.org/abs/2406.08210)

*Tom Wollschläger, Niklas Kemper, Leon Hetzel, Johanna Sommer, Stephan Günnemann*

*ICML 2024*

This paper conducts theoretical analyses of GNN models that explicitly use fragment information as inductive bias for molecular property prediction. Specifically, it proposes Fragment-WL test as an extension to the standard WL-test. Based on that, it proposes a new GNN architecture and a fragmentation with infinite vocabulary that improves the model expressiveness.


*2024-08-05*

#### [RAG-Ex: A Generic Framework for Explaining Retrieval Augmented Generation](https://dl.acm.org/doi/10.1145/3626772.3657660)

*Viju Sudhi, Sinchana Ramakanth Bhat, Max Rudat, Roman Teucher*

*SIGIR 2024 Short*

This paper proposes a framework to identify the key information (which could be viewed as `approximate' explanation) for LLMs' generation w.r.t. the input for tasks such as RAG or QA. Specifically, it introduces several perturbation approaches to distort the input, and identify the common part in the input of the cases when the LLM produces the correct answer.


*2024-07-22*

#### [Building Expressive and Tractable Probabilistic Generative Models: A Review](https://arxiv.org/abs/2402.00759)

*Sahil Sidheekh, Sriraam Natarajan*

*IJCAI 2024 Survey*

A Probabilistic Circuit is a computational graph that compactly encodes a probability distribution via factorizations and mixtures, which consists of three types of nodes - Sums, Products and Leaf Distributions. Each node in the graph computes a non-negative function, which can be interpreted as an unnormalized probability measure over a subset of random variables as the scope of the node. The computational graph is evaluated bottom-up recursively. This paper first introduces the building blocks, properties, learning methodologies and challenges for tractable probabilistic circuits, and then discusses hybrid techniques that merge tractable PCs with deep generative models to achieve the best of both worlds.


*2024-07-04*

#### [A Logic for Reasoning About Aggregate-Combine Graph Neural Networks](https://arxiv.org/abs/2405.00205)

*Pierre Nunn, Marco Sälzer, François Schwarzentruber, Nicolas Troquard*

*IJCAI 2024*

This paper proposes a modal logic in which counting modalities appear in linear inequalities, and each formula can be transformed into an equivalent graph neural network. In contrast, a class of GNNs can be transformed into a formula, thus making it more clear about the logical expressiveness of GNNs. It also shows that the satisfiability problem is PSPACE-complete.


*2024-06-11*

#### [Understanding Expressivity of GNN in Rule Learning](https://openreview.net/forum?id=43cYe4oogi)

*Haiquan Qiu, Yongqi Zhang, Yong Li, quanming yao*

*ICLR 2024*

This paper investigates the form of FOL rule structures that can be captured by a GNN model (but not in the reversed order, i.e., all possible rules that can be captured), which can be viewed as a lower bound of expressivity of certain types of GNNs. Besides, it proposes another GNN model that is proved to be able to capture more rule structures.


*2024-06-03*

#### [Logical Languages Accepted by Transformer Encoders with Hard Attention](https://openreview.net/forum?id=gbrHZq07mq)

*Pablo Barcelo, Alexander Kozachinskiy, Anthony Widjaja Lin, Vladimir Podolskii*

*ICLR 2024*

This paper investigates the formal languages that can be recognized by (1) Unique Hard Attention Transformers (UHAT) and (2) Average Hard Attention Transformers (AHAT). It obtains a characterization of which counting properties are expressible by UHAT and AHAT, in relation to regular languages.


*2024-05-31*

#### [Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals](https://openreview.net/forum?id=UMfcdRIotC)

*Yair Ori Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart*

*ICLR 2024*

To obtain explanation for predictions over causal graphs with order-faithfulness, this paper first proposes counterfactual generation as an approach, by prompting an LLM to change a specific text concept while keeping confounding concepts unchanged. As this approach is too costly for inference-time, it also presents a second matching-based method guided by an LLM at training-time and learns a dedicated embedding space.


*2024-05-29*

#### [A Multimodal Automated Interpretability Agent](https://arxiv.org/abs/2404.14394)

*Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, Antonio Torralba*

*Arxiv 2024*

This paper describes a multimodal automated interpretability agent that is equipped with a pre-trained vision-language model and a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. Not truly "interpretable" though.


*2024-05-28*

#### [Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning](https://openreview.net/forum?id=ZGNWW7xZ6Q)

*LINHAO LUO, Yuan-Fang Li, Reza Haf, Shirui Pan*

*ICLR 2024*

To enhance LLMs' reasoning trustfulness, this paper proposes a collaboration approach between an LLM and a KG. Specifically, it presents a planning-retrieval-reasoning framework, where relation paths grounded by the KG are firstly generated as 'plans' by optimizing a maximal probability target. Then the plans are used to retrieve reasoning paths from the KG, which are afterwards used by the LLM to conduct reasoning.


*2024-05-24*

#### [Expressivity of ReLU-Networks under Convex Relaxations](https://openreview.net/forum?id=awHTL3Hpto)

*Maximilian Baader, Mark Niklas Mueller, Yuhao Mao, Martin Vechev*

*ICLR 2024*

This paper studies the expressive power of finite ReLU neural networks by considering their various convex relaxations. It measures the networks ability to represent continuous piecewise linear (CPWL) functions.


*2024-05-22*

#### [REFACTOR: Learning to Extract Theorems from Proofs](https://openreview.net/forum?id=fgKjiVrm6u)

*Jin Peng Zhou, Yuhuai Wu, Qiyang Li, Roger Baker Grosse*

*ICLR 2024*

This paper aims to train a GNN for extracting a sub-component as the ``theorem'' from a proof tree. Specifically, given a proof tree with a set of nodes $V$, edges $E$, and node features $x_v$ which correspond to the name N and the proposition PROP associated with each node. The task is to output a subset of nodes $V_{target} ⊂ V$ that correspond to an embedded proof of a useful theorem. The problem is formulated as a node-level binary classification that predicts whether each node belongs to $V_{target}$. To solve the problem, it uses a GNN parametrized by $θ$ to take a graph with its node features as input, and outputs a scalar $\tilde{P}_v$ between 0 and 1 for each node $v \in V$, representing the probability belonging to $V_{target}$. The objective is a binary cross entropy loss between the node level probabilities and ground truth targets of a graph.


*2024-05-03*

#### [PEACH: Pretrained-embedding Explanation Across Contextual and Hierarchical Structure](https://arxiv.org/abs/2404.13645)

*Feiqi Cao, Caren Han, Hyunsuk Chung*

*IJCAI 2024*

This paper introduces PEACH, a tree-based explanation framework for text-based classification using pretrained contextual embeddings. Specifically, it obtains a pretrained embedding vector for each text document in the given corpus, then applies a feature extraction model (statistical or CNN-based) with decision tree algorithms to generate the tree-structured explanation.

