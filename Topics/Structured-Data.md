










*2025-12-22*

#### [An Index-based Approach for Efficient and Effective Web Content Extraction](https://arxiv.org/abs/2512.06641)

*Yihan Chen, Benfeng Xu, Xiaorui Wang, Zhendong Mao*

*Arxiv 2025*

Existing solutions for extracting relevant Web content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, this work introduces Index-based Web Content Extraction to reframe the extraction process to a discriminative task of index prediction. It partitions HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query.


*2025-12-19*

#### [Graph Queries from Natural Language using Constrained Language Models and Visual Editing](https://arxiv.org/abs/2512.00948)

*Benedikt Kantz, Kevin Innerebner, Peter Waldert, Stefan Lengauer, Elisabeth Lex, Tobias Schreck*

*ICKG 2025*

This work proposes a SPARQL query construction approach that enables users to specify prototype graphs in natural language and visually editing them. It converts natural language to prototype graphs by a two-step constrained language model generation based on semantically similarity within an ontology. The resulting prototype graph serves as building blocks for further user refinements within a dedicated visual query builder.


*2025-09-10*

#### [Improving Table Understanding with LLMs and Entity-Oriented Search](https://arxiv.org/abs/2508.17028)

*Thi-Nhung Nguyen, Hoang Ngo, Dinh Phung, Thuy-Trang Vu, Dat Quoc Nguyen*

*COLM 2025*

This paper introduces an entity-oriented search method to improve table understanding with LLMs. It leverages the semantic similarities between questions and table data, as well as the implicit relationships between table cells, to minimize the need for data preprocessing and keyword matching. Meanwhile, it focuses on table entities, ensuring that table cells are semantically tightly bound, thereby enhancing contextual clarity. It also uses Cypher, a graph query language for table understanding.


*2025-08-31*

#### [SPARQL in N3: SPARQL CONSTRUCT as a rule language for the Semantic Web](https://arxiv.org/abs/2508.13041)

*DÃ¶rthe Arndt, William Van Woensel, Dominik Tomaszuk*

*The 9th International Joint Conference on Rules and Reasoning*

This work adopts SPARQL CONSTRUCT queries as logic rules, enabling (1) an expressive, familiar SW rule language, and (2) general recursion, where queries can act on the results of others. It translates these queries to the Notation3 Logic (N3) rule language, allowing use of existing reasoning machinery with forward and backward chaining.


*2025-03-30*

#### [A Survey of Knowledge Graph Reasoning on Graph Types: Static, Dynamic, and Multi-Modal](https://ieeexplore.ieee.org/abstract/document/10577554)

*Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang*

*IEEE TAPMI*

This paper aggregates existing works for knowledge graph reasoning by categorizing graph types into static, dynamic and multi-modal ones, and grouping works by both the tast setting (i.e., inductive/transductive) and techniques (e.g., GNN).


*2024-10-07*

#### [LakeBench: A Benchmark for Discovering Joinable and Unionable Tables in Data Lakes](https://dl.acm.org/doi/10.14778/3659437.3659448)

*Yuhao Deng, Chengliang Chai, Lei Cao, Qin Yuan, Siyuan Chen, Yanrui Yu, Zhaoze Sun, Junyi Wang, Jiajun Li, Ziqi Cao, Kaisen Jin, Chi Zhang, Yuqing Jiang, Yuanfang Zhang, Yuping Wang, Ye Yuan, Guoren Wang, Nan Tang*

*VLDB 2024*

This paper introduces LakeBench, a large-scale table discovery benchmark, and evaluates the effectiveness, efficiency, and scalability of SOTA table join & union search methods.


*2024-10-05*

#### [Dependency-Aware Core Column Discovery for Table Understanding](https://link.springer.com/chapter/10.1007/978-3-031-47240-4_9)

*Jingyi Qiu, Aibo Song, Jiahui Jin, Tianbo Zhang, Jingyi Ding, Xiaolin Fang, Jianguo Qian*

*ISWC 2023*

This paper proposes Dependency-aware Core Column Discovery, an iterative method that uses a rough matching strategy to identify both inter-column dependencies and core columns. Unlike other methods, it does not require labeled data or contextual information. Besides, it can identify multiple core columns within a table, which are common for real-world tables.


*2024-08-22*

#### [Relationships are Complicated! An Analysis of Relationships Between Datasets on the Web]()

*Kate Lin, Tarfah Alrashed, Natasha Noy*

*ISWC 2024*

This paper first analyzes the tasks from the user's perspective, and proposes a taxonomy of relationships between datasets on the Web, including provenance-based, i.e., relationships between the datasets that share a common original dataset, and non-provenance-based relationships, i.e., connections between datasets based on content, topic, or task rather than their origin. Then it compares several methods for identifying dataset relationships, and shows that machine-learning methods (i.e., a classical gradient boosted decision trees classifier and a LLM-based classifier) using dataset metadata achieve a multi-class classification accuracy of 90%, outperforming schema.org and heuristics-based methods on a large corpus of datasets.


*2023-10-30*

#### [Incremental Tabular Learning on Heterogeneous Feature Space](https://dl.acm.org/doi/pdf/10.1145/3588698)

*Hanmo Liu, Shimin Di, Lei Chen*

*SIGMOD 2023*

Existing methods for incremental tabular learning usually assume that the new-coming datasets are from the same feature space as the old ones, i.e., homogeneous feature space, while in practice this is not always the case. To address this problem, this paper proposes an approach for incremental tabular learning in heterogeneous feature space. Specifically, it proposes different feature extractors for heterogeneous feature sets, as well as a measurement for the discrimination of these extractors. The overall loss function includes cross-entropy, regularization, and discriminative losses.


*2023-10-28*

#### [XTab: Cross-table Pretraining for Tabular Transformers](https://proceedings.mlr.press/v202/zhu23k.html)

*Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, Mahsa Shoaran*

*ICML 2023*

This paper proposes a cross-table pretrained transformer model which can handle tables from different domains and of various formats. Specifically, the proposed model architecture consists of a shared backbone transformer block, and data-specific featurizers and projection heads for different (pretraining) downstream tasks.

