





*2024-08-22*

#### [Relationships are Complicated! An Analysis of Relationships Between Datasets on the Web]()

*Kate Lin, Tarfah Alrashed, Natasha Noy*

*ISWC 2024*

This paper first analyzes the tasks from the user's perspective, and proposes a taxonomy of relationships between datasets on the Web, including provenance-based, i.e., relationships between the datasets that share a common original dataset, and non-provenance-based relationships, i.e., connections between datasets based on content, topic, or task rather than their origin. Then it compares several methods for identifying dataset relationships, and shows that machine-learning methods (i.e., a classical gradient boosted decision trees classifier and a LLM-based classifier) using dataset metadata achieve a multi-class classification accuracy of 90%, outperforming schema.org and heuristics-based methods on a large corpus of datasets.


*2023-10-30*

#### [Incremental Tabular Learning on Heterogeneous Feature Space](https://dl.acm.org/doi/pdf/10.1145/3588698)

*Hanmo Liu, Shimin Di, Lei Chen*

*SIGMOD 2023*

Existing methods for incremental tabular learning usually assume that the new-coming datasets are from the same feature space as the old ones, i.e., homogeneous feature space, while in practice this is not always the case. To address this problem, this paper proposes an approach for incremental tabular learning in heterogeneous feature space. Specifically, it proposes different feature extractors for heterogeneous feature sets, as well as a measurement for the discrimination of these extractors. The overall loss function includes cross-entropy, regularization, and discriminative losses.


*2023-10-28*

#### [XTab: Cross-table Pretraining for Tabular Transformers](https://proceedings.mlr.press/v202/zhu23k.html)

*Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, Mahsa Shoaran*

*ICML 2023*

This paper proposes a cross-table pretrained transformer model which can handle tables from different domains and of various formats. Specifically, the proposed model architecture consists of a shared backbone transformer block, and data-specific featurizers and projection heads for different (pretraining) downstream tasks.

