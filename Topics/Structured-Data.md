



*2023-10-28*

#### [XTab: Cross-table Pretraining for Tabular Transformers](https://proceedings.mlr.press/v202/zhu23k.html)

*Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, Mahsa Shoaran*

*ICML 2023*

This paper proposes a cross-table pretrained transformer model which can handle tables from different domains and of various formats. Specifically, the proposed model architecture consists of a shared backbone transformer block, and data-specific featurizers and projection heads for different (pretraining) downstream tasks.

