

*2023-03-13*

#### [Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries](https://dl.acm.org/doi/10.1145/3534678.3539472)

*Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, Jie Tang*

*KDD 2022*

This paper proposes a pre-training framework for logical query answering over knowledge graphs. It formulates a KG transformation strategy to turn relations into nodes thus eliminating the edge labels, and proposes a Mixture-of-Experts strategy to enhance the activation component of the transformers' feed-forward layers.


*2023-03-09*

#### [GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs](https://aclanthology.org/2022.emnlp-main.47/)

*Dong Yang, Peijun Qing, Yang Li, Haonan Lu, Xiaodong Lin*

*EMNLP 2022*

This paper proposes a probabilistic embedding model for FOL queries over knowledge graphs. It implements a Gamma mixture method to alleviate the non-closure problem on union operators, and enables query embeddings to have strict boundaries on the negation operator.


*2023-03-08*

#### [Embedding Logical Queries on Knowledge Graphs](https://proceedings.neurips.cc/paper/2018/hash/ef50c335cca9f340bde656363ebd02fd-Abstract.html)

*William L. Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, Jure Leskovec*

*NeurIPS 2018*

This paper introduces a method for conjunctive queries over knowledge graphs. The input contains a set of anchor nodes, and the target is to get the embedding of the query variable. The conjunctive query is formulated as a DAG, and two transformation operators are learned for projection and intersection in the embedding space, respectively.
