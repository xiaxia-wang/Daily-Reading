












*2023-12-19*

#### [RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability](https://arxiv.org/abs/2311.10947)

*Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie*

*Arxiv 2023*

This paper proposes to use LLMs for explaining recommendation models. Specifically, it considers two ways for aligning the LLM with the recommendation model, including (1) behavior alignment for which the LLM is trained to emulate the recommendation model’s predictive patterns—given a user’s profile as input, the LLM is fine-tuned to predict the items that the recommendation model would suggest to the user, and (2) intention alignment aims to incorporate the embeddings from the recommendation model to the LLM's prompts in a cross-modal manner.


*2023-12-14*

#### [ControlRec: Bridging the Semantic Gap between Language Model and Personalized Recommendation](https://arxiv.org/abs/2311.16441)

*Junyan Qiu, Haitao Wang, Zhaolin Hong, Yiping Yang, Qiang Liu, Xingxing Wang*

*Arxiv 2023*

An existing obstacle for using LLMs in the recommendation field is the representations in the semantic space used in recommendation such as user and item IDs, is distinct from natural language. To address this issue, this paper proposes two approaches to improve the alignment between user IDs and NL: (1) Heterogeneous Feature Matching (HFM) aligning item description (NL) with the corresponding ID or user’s next preferred ID based on their interaction sequence, and (2) Instruction Contrastive Learning (ICL) effectively merging these two crucial data sources by contrasting probability distributions of output sequences generated by diverse tasks.


*2023-12-10*

#### [Recommender Systems with Generative Retrieval](https://arxiv.org/abs/2305.05065)

*Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H. Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Samost, Maciej Kula, Ed H. Chi, Maheswaran Sathiamoorthy*

*NeurIPS 2023*

Modern recommender systems perform large-scale retrieval by embedding queries and item candidates in the same unified space, followed by approximate nearest neighbor search to select top candidates given a query embedding. In contrast, this paper enables the retrieval model to autoregressively decode the identifiers of the target candidates by creating semantically meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic IDs for items in a user session, a Transformer-based seq2seq model is trained to predict the Semantic ID of the next item that the user will interact with.


*2023-12-09*

#### [Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction](https://dl.acm.org/doi/10.1145/3583780.3615089)

*Fangye Wang, Hansu Gu, Dongsheng Li, Tun Lu, Peng Zhang, Ning Gu*

*CIKM 2023*

CTR prediction aims to estimate the probability of a user clicking on a recommended item or an advertisement on a web page. This paper proposes a Gated Cross Network (GCN) that captures explicit high-order feature interactions and dynamically filters important interactions with an information gate in each order. Then it also uses a Field-level Dimension Optimization (FDO) approach to learn condensed dimensions for each field based on their importance.


*2023-11-30*

#### [LLM4Vis: Explainable Visualization Recommendation using ChatGPT](https://arxiv.org/abs/2310.07652)

*Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, Yong Wang*

*EMNLP 2023*

Visualization recommendation aims to find the most appropriate type of visualization approach for a given dataset. To achieve this, this paper proposes a prompt-based pipeline, which involves feature description, example selection, and explanation generation (by LLM). Then in the test phase, it searches for the top-k nearest examples from the example set, and combines them with the input test example to generate the visualization recommendation.


*2023-11-23*

#### [Session-based Recommendations with Recurrent Neural Networks](https://github.com/hidasib/GRU4Rec)

*Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk*

*ICLR 2016*

This is an early work that uses RNN, specifically, GRU-based network models, to simulate the long history of user-item interactions for recommendation.


*2023-11-19*

#### [LLMRec: Large Language Models with Graph Augmentation for Recommendation](https://arxiv.org/abs/2311.00423)

*Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang*

*WSDM 2024*

This paper proposes to use LLM to generate extra information for augmenting the user-item interation graph thus improving the recommendation performance. Specifically, it applies LLM to (1) generate pseudo user-item edges as extra positive/negative examples, (2) generate user profiles such as name, gender from the recommendation history, and (3) encode the generated information as auxiliary features used as input for the recommender.


*2023-11-11*

#### [How Expressive are Graph Neural Networks in Recommendation?](https://dl.acm.org/doi/10.1145/3583780.3614917)

*Xuheng Cai, Lianghao Xia, Xubin Ren, Chao Huang*

*CIKM 2023*

This paper studies the problem of measuring the expressiveness of GNN models in the recommendation context. Specifically, it first introduces expressiveness metrics on three levels, i.e., graph level, node level, and link level. In particular, it proposes a topological closeness metric to evaluate GNNs’ ability to capture the structural distance between nodes, which closely aligns with the recommendation objective.


*2023-11-06*

#### [Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach](https://arxiv.org/abs/2305.07001)

*Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, Ji-Rong Wen*

*Arxiv 2023*

This paper proposes to apply LLMs for the sequence recommendation task. It first proposes a general instruction format to describe the preference, intention, task form and the context for recommendation. Then it manually designs 39 instruction templates and uses them to generate a set of user-personalized instructions data.


*2023-11-05*

#### [Prompt Distillation for Efficient LLM-based Recommendation](https://dl.acm.org/doi/10.1145/3583780.3615017)

*Lei Li, Yongfeng Zhang, Li Chen*

*CIKM 2023*

This work explores to use LLM for recommendation by prompt distillation. Generally, recommendation requires user and item IDs as the input for the LLM, usually used as prompts. This paper proposes to learn a shorter dense vector as prompt to replace the original longer sparse prompt. Concretely, it appends a set of vectors at the beginning of an input sample that already filled in a discrete prompt template, and allow the vectors to be shared by the samples of the same recommendation task. It evaluates the proposed method for sequence recommendation, top-N recommendation, and generation of explanations.

